{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elenadigrigoli/FDS-project/blob/main/deep_learning_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evHwS_3q-HvH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Iw3ydPg3sRg"
      },
      "outputs": [],
      "source": [
        "# Function to de-normalize images\n",
        "def denormalize_image(tensor, mean, std):\n",
        "    tensor = tensor.clone()  # Create a copy to avoid modifying the original\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)  # Reverse the normalization operation\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "19zC0NIp39Cp"
      },
      "outputs": [],
      "source": [
        "# Function to visualize a batch of images\n",
        "def show_images_from_loader(loader, parent_dataset, mean, std):\n",
        "    \"\"\"\n",
        "    Display a batch of preprocessed images with their labels.\n",
        "\n",
        "    Args:\n",
        "        loader (DataLoader): The DataLoader from which to extract a batch.\n",
        "        parent_dataset (Dataset): The parent dataset (e.g., train_dataset_full).\n",
        "        mean (list): Mean used for normalization.\n",
        "        std (list): Standard deviation used for normalization.\n",
        "    \"\"\"\n",
        "    classes = parent_dataset.classes  # Get the class names from the parent dataset\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)  # Retrieve a batch\n",
        "\n",
        "    # De-normalize the batch\n",
        "    images = images.clone()\n",
        "    for i in range(images.size(0)):\n",
        "        images[i] = denormalize_image(images[i], mean, std)\n",
        "\n",
        "    # Convert to numpy format for matplotlib\n",
        "    images = images.numpy().transpose((0, 2, 3, 1))  # From CxHxW to HxWxC\n",
        "\n",
        "    # Display the first 4 images in the batch\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i >= len(images):  # Avoid errors if there are fewer than 4 images in the batch\n",
        "            break\n",
        "        ax.imshow(np.clip(images[i], 0, 1))  # Ensure values are in the range [0, 1]\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(f\"Label: {classes[labels[i]]}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(dataset_path, train_split=0.7, val_split=0.2, test_split=0.1, batch_size=32):\n",
        "\n",
        "    # Transform images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "    # Compute sets size\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(train_split * dataset_size)\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    # Split the dataset\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    # Create DataLoader for each part of the data\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN8D6MPsxzk9E50fM4shAEc",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
