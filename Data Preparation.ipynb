{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import tensorflow as tf\n",
    "from data_preparation_utils import *\n",
    "from machine_learning_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base folder where all your image classes are stored\n",
    "folder = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_data = datasets.ImageFolder(folder, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check min and max height and width of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Width: 71, Max Width: 6283\n",
      "Min Height: 51, Max Height: 7786\n"
     ]
    }
   ],
   "source": [
    "check_max_min_dimensions(start_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing the images to 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.Resize((256, 256), antialias=True), transforms.ToTensor()])\n",
    "data = datasets.ImageFolder(root = folder, transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all images have the same dimensions and the same number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have the same shape: torch.Size([256, 256]).\n",
      "All images have 3 channels.\n"
     ]
    }
   ],
   "source": [
    "check_dimensions(data)\n",
    "check_channels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of samples in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class battery: 944 images\n",
      "Class biological: 983 images\n",
      "Class cardboard: 1810 images\n",
      "Class clothes: 5323 images\n",
      "Class glass: 3039 images\n",
      "Class metal: 994 images\n",
      "Class paper: 1650 images\n",
      "Class plastic: 1915 images\n",
      "Class shoes: 1977 images\n",
      "Class trash: 772 images\n"
     ]
    }
   ],
   "source": [
    "count_images_per_class(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation to bring all the classes to at least 1500 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 556 images for class: battery\n",
      "Augmenting 517 images for class: biological\n",
      "Class cardboard already has 1810 images, no augmentation needed.\n",
      "Class clothes already has 5323 images, no augmentation needed.\n",
      "Class glass already has 3039 images, no augmentation needed.\n",
      "Augmenting 506 images for class: metal\n",
      "Class paper already has 1650 images, no augmentation needed.\n",
      "Class plastic already has 1915 images, no augmentation needed.\n",
      "Class shoes already has 1977 images, no augmentation needed.\n",
      "Augmenting 728 images for class: trash\n",
      "Data augmentation completed.\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"data\"\n",
    "output_dir = \"augmented_data\"\n",
    "target_size = 1500\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "\n",
    "for class_name in os.listdir(input_dir):\n",
    "    input_class_dir = os.path.join(input_dir, class_name)\n",
    "\n",
    "    num_images = len([img for img in os.listdir(input_class_dir) if img.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "    if num_images < target_size:\n",
    "        augment_images(class_name, input_class_dir, num_images, target_size, augmentations=augmentations)\n",
    "    else:\n",
    "        print(f\"Class {class_name} already has {num_images} images, no augmentation needed.\")\n",
    "\n",
    "print(\"Data augmentation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class battery -> Train: 1200 | Test: 300\n",
      "Class biological -> Train: 1200 | Test: 300\n",
      "Class cardboard -> Train: 1448 | Test: 362\n",
      "Class clothes -> Train: 4258 | Test: 1065\n",
      "Class glass -> Train: 2431 | Test: 608\n",
      "Class metal -> Train: 1200 | Test: 300\n",
      "Class paper -> Train: 1320 | Test: 330\n",
      "Class plastic -> Train: 1532 | Test: 383\n",
      "Class shoes -> Train: 1581 | Test: 396\n",
      "Class trash -> Train: 1200 | Test: 300\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "original_dataset_dir = \"data\"\n",
    "output_dir = \"final dataset\"\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "\n",
    "# Create train/test directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "test_size = 0.2 \n",
    "random_state = 42\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset(original_dataset_dir, train_dir, test_dir, test_size=test_size, random_state=random_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
